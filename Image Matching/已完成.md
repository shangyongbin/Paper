- **Lift: Learned in variant feature transform.** 【2016】
- **L2-net: Deep learning of disriminative patch descriptor in euclidean space.** 【2017】
- **Lf-net: learning local features from images.** 【2018】【self-supervised】
- **Rf-net: An end-to-end image matching network based on receptive field.** 【2019】
- **Superpoint: Self-supervised interest point detection and description.** 【2018】【self-supervised】【好】
- **R2d2: Repeatable and reliable detector and descriptor.** 【2019】【self-supervised】
- **D2-net: A trainable cnn for joint detection and description of local features.** 【2019】
- **D2D: Keypoint Extraction with Describe to Detect Approach** 【2020】
- **ELF: Embedded Localisation of Features in pre-trained CNN** 【ICCV 2019】





## 小结

:heavy_check_mark:**Lift: Learned in variant feature transform.** 【2016】



:heavy_check_mark:**L2-net: Deep learning of disriminative patch descriptor in euclidean space.** 【2017】





:heavy_check_mark:**Lf-net: learning local features from images.** 【2018】【self-supervised】





:heavy_check_mark:**Rf-net: An end-to-end image matching network based on receptive field.** 【2019】





:heavy_check_mark:**Superpoint: Self-supervised interest point detection and description.** 【2018】【self-supervised】

- 亮点

  - **detect-and-describe**
  - self-supervised (**self-training**)
  - **homographic adaptation**

- 结构

  ![image-20210809171335945](D:\workspace\Paper\Image Matching\已完成.assets\image-20210809171335945.png)

  ![image-20210829144752794](D:\workspace\Paper\Image Matching\已完成.assets\image-20210829144752794.png)

  ![image-20210829151755032](D:\workspace\Paper\Image Matching\已完成.assets\image-20210829151755032.png)

  ![image-20210829153326944](D:\workspace\Paper\Image Matching\已完成.assets\image-20210829153326944.png)

  |     name      | ksize | stride | padding | **channel** |       size       |  rf  |      | start |      | startn |
  | :-----------: | :---: | :----: | :-----: | :---------: | :--------------: | :--: | :--: | :---: | :--: | :----: |
  |               |       |   1    |         |      3      |   $H \times W$   |  1   |      |   0   |      |        |
  | `conv1a-bn1a` |   3   |   1    |    1    |     64      |                  |  3   |  +2  |       |  +0  |        |
  | `conv1b-bn1b` |   3   |   1    |    1    |     64      |                  |  5   |  +2  |       |  +0  |        |
  |   `maxpool`   |   2   |   2    |         |             | $H/2 \times W/2$ |  6   |      |       | +0.5 |        |
  | `conv2a-bn2a` |   3   |   1    |    1    |     64      |                  |  10  |  +4  |       |  +0  |        |
  | `conv2b-bn2b` |   3   |   1    |    1    |     64      |                  |  14  |  +4  |       |  +0  |        |
  |   `maxpool`   |   2   |   2    |         |             | $H/4 \times W/4$ |  16  |      |       | +0.5 |        |
  |   `conv3a`    |   3   |   1    |    1    |     128     |                  |  24  |  +8  |       |  +0  |        |
  |   `conv3b`    |   3   |   1    |    1    |     128     |                  |  32  |  +8  |       |  +0  |        |
  |   `maxpool`   |   2   |   2    |         |             | $H/8 \times W/8$ |  36  |      |       | +0.5 |        |
  |   `conv4a`    |   3   |   1    |    1    |     128     |                  |  52  | +16  |       |  +0  |        |
  |   `conv4b`    |   3   |   1    |    1    |     128     |                  |  68  | +16  |       |  +0  |        |
  |               |       |        |         |             |                  |      |      |       |      |        |
  |   `convPa`    |   3   |   1    |    1    |     256     | $H/8 \times W/8$ |  84  | +16  |       |  +0  |        |
  |   `convPb`    |   1   |   1    |         |     65      |                  |  84  |      |       |      |   8n   |
  |               |       |        |         |             |                  |      |      |       |      |        |
  |   `convDa`    |   3   |   1    |    1    |     256     | $H/8 \times W/8$ |  84  | +16  |       |  +0  |        |
  |   `convDb`    |   1   |   1    |         |     256     |                  |  84  |      |       |      |   8n   |

  - 上采样层：高计算量、棋盘效应
    - 先通过a channel-wise softmax，the dustbin dimension is removed；然后reshape$\mathbb{R}^{H_c \times W_c \times 64} \Rightarrow \mathbb{R}^{H \times W}$
      - each pixel of the output corresponds to a probability of “**pointness**” for that pixel in the input.
    - 先通过Bi-Cubic Interpolate，得到 a semi-dense grid（one every 8 pixels） of descriptors $\mathbb{R}^{H \times W \times D}$；再通过L2-normalization变为单位长度。

- 训练

  - MagicPoint：Synthetic pre-training

    - 数据集：Synthetic Shapes
      - ground truth: labeled
    - 网络：Encoder+Interest Point Decoder
    - 损失：-logp

  - Train

    - 数据集：[MS-COCO 2014](https://cocodataset.org/#home)  training dataset split which has 80,000 images. The images are sized to a resolution of 240 × 320 and converted to grayscale.

      - ground truth: Homographic Adaptation with $N_h = 100$

        ![image-20210810153718368](D:\workspace\Paper\Image Matching\已完成.assets\image-20210810153718368.png)

    - 损失
      $$
      \mathcal{L}(\mathcal{X}, \mathcal{X'}, \mathcal{D}, \mathcal{D'}; \mathcal{Y}, \mathcal{Y'}, S) = 
      \mathcal{L}_p(\mathcal{X}, \mathcal{Y}) + 
      \mathcal{L}_p(\mathcal{X'}, \mathcal{Y'}) +
      \mathcal{L}_d(\mathcal{D}, \mathcal{D'}, S)
      
      \\
      
      \mathcal{L}_p(\mathcal{X}, \mathcal{Y}) = 
      \frac{1}{H_c W_c}
      \sum_{h=1 \\ w=1}^{H_c, W_c}
      l_p(\mathbb{x}_{hw}; y_{hw}) 
      
      \\
      
      l_p(\mathbb{x}_{hw}; y_{hw}) = 
      -log(
      \frac
      { exp(\mathbb{x}_{hwy}) }
      { \sum_{k=1}^{65} exp(\mathbb{x}_{hwk}) }
      )
      $$
      ![image-20210810192641972](D:\workspace\Paper\Image Matching\已完成.assets\image-20210810192641972.png)

:heavy_check_mark:**R2d2: Repeatable and reliable detector and descriptor.** 【2019】【self-supervised】

- 未搞懂
  - ~~数据集中的aflow~~：表示`img1[x,y]`对应`img2[aflow[x,y]]`
  - 损失中的listwise ranking loss: AP loss不懂
  - L2Net结构的设计：需要看L2Net论文

- 总结
  - detect-and-describe
  - repeatable, reliable（discriminative）
  - keypoint detector（repeatability）：unsupervised loss， descriptor：a listwise ranking loss

- 结构（类似**L2Net**）

  ![image-20210726125350189](D:\workspace\Paper\Image Matching\已完成.assets\image-20210726125350189-1630121102085.png)

  ![image-20210829144752794](D:\workspace\Paper\Image Matching\已完成.assets\image-20210829144752794.png)

  ![image-20210829145020568](D:\workspace\Paper\Image Matching\已完成.assets\image-20210829145020568.png)

  |         name          | ksize | stride | padding | dilation | **channel** |     size     |  rf  |      |  t   |  tn  |
  | :-------------------: | :---: | :----: | :-----: | :------: | :---------: | :----------: | :--: | :--: | :--: | :--: |
  |                       |       |   1    |         |          |      3      | $H \times W$ |  1   |      |      |      |
  |  `conv1a-bn1a-relu`   |   3   |   1    |    1    |          |     32      |              |  3   |  +2  |  +0  |      |
  |  `conv1b-bn1b-relu`   |   3   |   1    |    1    |          |     32      |              |  5   |  +2  |  +0  |      |
  |  `conv2a-bn2a-relu`   |   3   |   1    |    1    |          |     64      |              |  7   |  +2  |  +0  |      |
  |  `conv2b-bn2b-relu`   |   3   |   1    |    2    |    2     |     64      |              |  11  |  +4  |  +0  |      |
  |   `conv3a-bn-relu`    |   3   |   1    |    2    |    2     |     128     |              |  15  |  +4  |  +0  |      |
  |   `conv3b-bn-relu`    |   3   |   1    |    4    |    4     |     128     |              |  23  |  +8  |  +0  |      |
  |      `conv4a-bn`      |   2   |   1    |    2    |    4     |     128     |              |  27  |  +4  |  +0  |      |
  |      `conv4b-bn`      |   2   |   1    |    4    |    8     |     128     |              |  35  |  +8  |  +0  |      |
  |       `conv4c`        |   2   |   1    |    8    |    16    |     128     | $H \times W$ |  51  | +16  |  +0  |      |
  |                       |       |        |         |          |             |              |      |      |      |      |
  |  `l2 normalization`   |       |        |         |          |     128     | $H \times W$ |      |  51  |      |  n   |
  |                       |       |        |         |          |             |              |      |      |      |      |
  | `element-wise square​` |       |        |         |          |             |              |      |      |      |      |
  |    `ureliability`     |   1   |   1    |         |          |      2      |              |      |      |      |      |
  |       `softmax`       |       |        |         |          |      1      | $H \times W$ |      |  51  |      |  n   |
  |                       |       |        |         |          |             |              |      |      |      |      |
  | `element-wise square` |       |        |         |          |             |              |      |      |      |      |
  |   `urepeatability`    |   1   |   1    |         |          |      1      |              |      |      |      |      |
  |      `softplus`       |       |        |         |          |             |              |      |      |      |      |
  |     `x / (1 + x)`     |       |        |         |          |      1      | $H \times W$ |      |  51  |      |  n   |
  |                       |       |        |         |          |             |              |      |      |      |      |

- 训练

  - 数据集

    - 数据

      web_images

      ```txt
      web_images.py
          RandomWebImages(Dataset)：定义了Web数据类
              root -> 'data/revisitop1m'
              imgs -> ['003c52e83c7adde3168cdef74f60.jpg',...]
              nimg -> 3125
      
              get_image(idx) -> PIL.Image
      ```

      aachen_db_images

      aachen_style_transfer_pairs

      aachen_flow_pairs

      ```txt
      aachen.py
          AachenImages_DB(AachenImages)
              root -> 'data/aachen'
              imgs -> ['db/1.jpg',...]
              nimg -> 4479
      
              get_image(idx) -> PIL.Image
          
          AachenPairs_StyleTransferDayNight (AachenImages_DB, StillPairDataset)
              root -> 'data/aachen'
              imgs -> ['db/1.jpg',...]
              nimg -> 8115
              image_pairs -> [(origin_img_idx, st_img_idx),...] 原图像索引与风格迁移后图像索引
              npairs -> 3636
      
              get_image(idx) -> PIL.Image
              get_pair(idx, 'aflow') -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
      
          AachenPairs_OpticalFlow (AachenImages_DB, PairDataset)
              root -> 'data/aachen'
              root_flow -> 'data/aachen/optical_flow'
              imgs -> ['db/1.jpg',...]
              nimg -> 4479
              image_pairs -> [(img1_idx, img2_idx),...] 有光流的db图像对索引
              npairs -> 4770
      
              get_image(idx) -> PIL.Image
              get_pair(idx, 'aflow') -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
      ```

    - 图像对类

      ```txt
      pair_dataset.py
          SyntheticPairDataset(PairDataset)：处理数据类web_images和aachen_db_images
              属性：
                   dataset —— 数据类
                   npairs —— 图像对数目
              方法：
                   get_image(idx)
                   get_pair(i) -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
      
          TransformedPairs(PairDataset): 处理aachen_style_transfer_pairs
              属性：
                   dataset —— 数据类
                   npairs —— 图像对数目
              方法：
                   get_pair(i) -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
      ```

    - 合成类

          pair_dataset.py
              CatPairDataset(CatDataset)
                  属性:
                       datasets
                       offsets
                       nming
                       pair_offsets
                       npairs
                  方法: 
                       get_pair(idx)

  - 输入、输出

    `img1(batch,3,192,193)`,`img2(batch,3,192,193)`,`aflow(batch,2,192,192)`,`mask(batch,192,192)`

    `(2)repeatability(batch,1,192,192), (2)reliability(batch,1,192,192), (2)descriptor(batch,128,192,192), aflow(batch,2,192,192), mask(batch,192,192)`

  - 损失

    - repeatability: `(2)repeatability(batch,1,192,192), aflow(batch,2,192,192)`

    ![image-20210726125505752](D:\workspace\Paper\Image Matching\已完成.assets\image-20210726125505752.png)

    ```txt
            sali1, sali2 = repeatability
            grid = FullSampler._aflow_to_grid(aflow) # (2, 192, 192, 2):将W和H方向归一化至[-1,1]
            sali2 = F.grid_sample(sali2, grid, mode='bilinear', padding_mode='border')
    ```

    - PixelAPLoss: `(2)descriptor(batch,128,192,192), aflow(batch,2,192,192)`
    - 

:heavy_check_mark:**D2-net: A trainable cnn for joint detection and description of local features.** 【2019】

- 结构

  ![image-20210726135108726](D:\workspace\Paper\Image Matching\已完成.assets\image-20210726135108726-1630121242278.png)

- 训练

  - 数据集
  - 损失

  

:heavy_check_mark:**D2D: Keypoint Extraction with Describe to Detect Approach** 【2020】

- describe-then-detect, 给出了关键点的一种基于描述符的定义

- 结构

  - keypoints should be image points that have the potential of being repeatably detected under different imaging conditions.

  - define the saliency in relative terms （the other descriptors in the neighbourhood）and in absolute terms （the information content of the descriptor）.

    - A point in an image has **a high absolute saliency** if its corresponding descriptor is highly **informative**.

      ![image-20210803152302488](D:\workspace\Paper\Image Matching\已完成.assets\image-20210803152302488.png)

    - A point in an image has **a high relative saliency** if its corresponding descriptor is highly **discriminative** in its spatial neighbourhood.

      ![image-20210803152449358](D:\workspace\Paper\Image Matching\已完成.assets\image-20210803152449358.png)

- 训练

  无需重新训练，只需使用现有的descriptors。



**:heavy_check_mark: ELF: Embedded Localisation of Features in pre-trained CNN**

- 总结
  - 直接从feature map得到特征点
- 方法
  1. compute a saliency map (feature gradient) — 特征图相对于输入图像的梯度
  2. automatically threshold the map
  3. run NMS for local detection

