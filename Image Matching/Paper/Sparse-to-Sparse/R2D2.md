19年CVPR，自监督。

## Conclude

- **detect-and-describe** + **self-supervised**
  - 传统范式： a two-stage pipeline: detect-then-describe + trained with metric learning losses
  - jointly learn keypoint detection (**repeatability**) and **description** together with a predictor of the local descriptor discriminativeness (**reliability**)
- **repeatable, reliable**
- a novel unsupervised loss （keypoint detector）， a listwise ranking loss（descriptor）
  - keypoint detector:  sparse, repeatable
  - descriptors: reliability (discriminativeness)



## Motivation

- 显著区域（salient, repeatable region）可能没有区分性（discriminative），损害描述符的性能。
  
  - 现有的keypoint detector只关注于repeatability
    
  - descriptors should be learned only in regions for which matching can be performed with high confidence.
  
    ![image-20210829131311567](D:\workspace\Paper\Image Matching\Paper\Sparse-to-Sparse\R2D2.assets\image-20210829131311567.png)

## 介绍

- detect, describe

  - 关键点
    - covariance with respect to camera viewpoint changes
    - invariance with respect to photometric transformations
  - 描述符
    - 基于histograms of local gradients

- 现有技术

  - 检测器

    - handcrafted methods

      - limited by the a priori knowledge researchers have about the tasks at hand.

      - let a deep network discover automatically which feature extraction process and representation are most suited to the data.

    - data-driven approaches

      - keypoint detector只关注于repeatability

        SuperPoint（输出pointness响应图）， D2-net（输出特征图，每一层都是响应图）， Lf-net（），Quad-networks（）， Lift（）

  - 描述符

    - metric learning techniques

      They are trained on the repeatable locations provided by the detector.



## Method

### 框架

网络框架基于L2Net。

![image-20210726125350189](D:\workspace\Paper\Image Matching\Paper\Sparse-to-Sparse\R2D2.assets\image-20210726125350189.png)

|         name          | ksize | stride | padding | dilation | **channel** |     size     |  rf  |      |  t   |  tn  |
| :-------------------: | :---: | :----: | :-----: | :------: | :---------: | :----------: | :--: | :--: | :--: | :--: |
|                       |       |   1    |         |          |      3      | $H \times W$ |  1   |      |      |      |
|  `conv1a-bn1a-relu`   |   3   |   1    |    1    |          |     32      |              |  3   |  +2  |  +0  |      |
|  `conv1b-bn1b-relu`   |   3   |   1    |    1    |          |     32      |              |  5   |  +2  |  +0  |      |
|  `conv2a-bn2a-relu`   |   3   |   1    |    1    |          |     64      |              |  7   |  +2  |  +0  |      |
|  `conv2b-bn2b-relu`   |   3   |   1    |    2    |    2     |     64      |              |  11  |  +4  |  +0  |      |
|   `conv3a-bn-relu`    |   3   |   1    |    2    |    2     |     128     |              |  15  |  +4  |  +0  |      |
|   `conv3b-bn-relu`    |   3   |   1    |    4    |    4     |     128     |              |  23  |  +8  |  +0  |      |
|      `conv4a-bn`      |   2   |   1    |    2    |    4     |     128     |              |  27  |  +4  |  +0  |      |
|      `conv4b-bn`      |   2   |   1    |    4    |    8     |     128     |              |  35  |  +8  |  +0  |      |
|       `conv4c`        |   2   |   1    |    8    |    16    |     128     | $H \times W$ |  51  | +16  |  +0  |      |
|                       |       |        |         |          |             |              |      |      |      |      |
|  `l2 normalization`   |       |        |         |          |     128     | $H \times W$ |      |  51  |      |  n   |
|                       |       |        |         |          |             |              |      |      |      |      |
| `element-wise square​` |       |        |         |          |             |              |      |      |      |      |
|    `ureliability`     |   1   |   1    |         |          |      2      |              |      |      |      |      |
|       `softmax`       |       |        |         |          |      1      | $H \times W$ |      |  51  |      |  n   |
|                       |       |        |         |          |             |              |      |      |      |      |
| `element-wise square` |       |        |         |          |             |              |      |      |      |      |
|   `urepeatability`    |   1   |   1    |         |          |      1      |              |      |      |      |      |
|      `softplus`       |       |        |         |          |             |              |      |      |      |      |
|     `x / (1 + x)`     |       |        |         |          |      1      | $H \times W$ |      |  51  |      |  n   |
|                       |       |        |         |          |             |              |      |      |      |      |

### 数据集

- 数据

  - web_images

    ```txt
    web_images.py
        RandomWebImages(Dataset)：定义了Web数据类
            root -> 'data/revisitop1m'
            imgs -> ['003c52e83c7adde3168cdef74f60.jpg',...]
            nimg -> 3125
    
            get_image(idx) -> PIL.Image
    ```

    aachen_db_images

    aachen_style_transfer_pairs

    aachen_flow_pairs

    ```txt
    aachen.py
        AachenImages_DB(AachenImages)
            root -> 'data/aachen'
            imgs -> ['db/1.jpg',...]
            nimg -> 4479
    
            get_image(idx) -> PIL.Image
        
        AachenPairs_StyleTransferDayNight (AachenImages_DB, StillPairDataset)
            root -> 'data/aachen'
            imgs -> ['db/1.jpg',...]
            nimg -> 8115
            image_pairs -> [(origin_img_idx, st_img_idx),...] 原图像索引与风格迁移后图像索引
            npairs -> 3636
    
            get_image(idx) -> PIL.Image
            get_pair(idx, 'aflow') -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
    
        AachenPairs_OpticalFlow (AachenImages_DB, PairDataset)
            root -> 'data/aachen'
            root_flow -> 'data/aachen/optical_flow'
            imgs -> ['db/1.jpg',...]
            nimg -> 4479
            image_pairs -> [(img1_idx, img2_idx),...] 有光流的db图像对索引
            npairs -> 4770
    
            get_image(idx) -> PIL.Image
            get_pair(idx, 'aflow') -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
    ```

  - 图像对类

    ```txt
    pair_dataset.py
        SyntheticPairDataset(PairDataset)：处理数据类web_images和aachen_db_images
            属性：
                 dataset —— 数据类
                 npairs —— 图像对数目
            方法：
                 get_image(idx)
                 get_pair(i) -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
    
        TransformedPairs(PairDataset): 处理aachen_style_transfer_pairs
            属性：
                 dataset —— 数据类
                 npairs —— 图像对数目
            方法：
                 get_pair(i) -> Image, Image, {'aflow':(H,W,2), 'flow':(H,W,2)}
    ```

  - 合成类

        pair_dataset.py
            CatPairDataset(CatDataset)
                属性:
                     datasets
                     offsets
                     nming
                     pair_offsets
                     npairs
                方法: 
                     get_pair(idx)

- 输入、输出

`img1(batch,3,192,193)`,`img2(batch,3,192,193)`,`aflow(batch,2,192,192)`,`mask(batch,192,192)`

`(2)repeatability(batch,1,192,192), (2)reliability(batch,1,192,192), (2)descriptor(batch,128,192,192), aflow(batch,2,192,192), mask(batch,192,192)`

输入：相同场景的两个图片$I$和$I'\in R^{H\times W}$，$U \in R^{H\times W \times 2}$是groundtruth

- 如果第一个图$I$的 $(i,j)$对应第二个图$I'$的 $(i',j')$，那么$U_{i,j}=(i',j')$。
- 实际中，$U$的估计方法：
  - using existing **optical flow** or **stereo matching algorithms** if $I$ and $I'$ are **natural images**
  - can be obtained exactly if  $I'$ was generated synthetically with a known transformation, e.g. an **homography**.
- **all local maxima in $S$ correspond to the ones in$ S'_U$ .**

输出：

- dense local descriptors (one for each pixel) $X_1, X_2 \in R^{H\times W \times D}$
- repeatability confidence map $S_1, S_2 \in [0,1]^{H \times W}$, $S'_U $ from $I$ warped according to U
- reliability confidence map $R_1, R_2 \in [0,1]^{H \times W}$

### 损失

**repeatability**：`(2)repeatability(batch,1,192,192), aflow(batch,2,192,192)`

![image-20210726125505752](D:\workspace\Paper\Image Matching\Paper\Sparse-to-Sparse\R2D2.assets\image-20210726125505752.png)

        sali1, sali2 = repeatability
        grid = FullSampler._aflow_to_grid(aflow) # (2, 192, 192, 2):将W和H方向归一化至[-1,1]
        sali2 = F.grid_sample(sali2, grid, mode='bilinear', padding_mode='border')

**descriptor + reliability**： `(2)descriptor(batch,128,192,192), aflow(batch,2,192,192)`

```python
PixelAPLoss()
    def forward(self, descriptors, aflow, **kw):
        # subsample things
        scores, gt, msk, qconf = self.sampler(descriptors, kw.get('reliability'), aflow) # (3200,3281), (3200,3281), (8,400), (8,400)

        # compute pixel-wise AP
        n = qconf.numel() # 3200
        if n == 0: return 0 
        scores, gt = scores.view(n,-1), gt.view(n,-1) # (3200, 3281), (3200,3281)
        ap = self.aploss(scores, gt).view(msk.shape) # (8, 400)

        pixel_loss = self.loss_from_ap(ap, qconf)

        loss = pixel_loss[msk].mean()
        return loss
```

```python
APLoss()
    def forward(self, x, label):
        assert x.shape == label.shape # N x M 3200x3281
        return self.compute_AP(x, label)
    def compute_AP(self, x, label):
        N, M = x.shape
        if self.euc:  # euclidean distance in same range than similarities
            x = 1 - torch.sqrt(2.001 - 2*x)

        # quantize all predictions
        q = self.quantizer(x.unsqueeze(1)) # (3200,1,3281) -> (3200,40,3281)
        q = torch.min(q[:,:self.nq], q[:,self.nq:]).clamp(min=0) # N x Q x M (3200,20,3281)

        nbs = q.sum(dim=-1) # number of samples  N x Q = c # (3200,20)
        rec = (q * label.view(N,1,M).float()).sum(dim=-1) # nb of correct samples = c+ N x Q # (3200,20)
        prec = rec.cumsum(dim=-1) / (1e-16 + nbs.cumsum(dim=-1)) # precision #  (3200,20)
        rec /= rec.sum(dim=-1).unsqueeze(1) # norm in [0,1] #  (3200,20)

        ap = (prec * rec).sum(dim=-1) # per-image AP
        return ap # (3200,)

```



### 测试

- we run the trained network multiple times on the input image at different scales starting from the original scale, and downsampling by $ 2^{1/4}$ each time until the image is smaller than 128 pixels.

- For each scale, we find **local maxima in S** and gather **descriptors from X at corresponding locations**.
- Finally, we keep a shortlist of the best K descriptors over all scales where the descriptor score is computed as product $S_{ij}R_{ij}$ , i.e., requiring high values for both repeatability and reliability



## Result

